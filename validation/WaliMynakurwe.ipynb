{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bebd650f-5d0a-453f-9bc6-0fba7fd3ceea",
   "metadata": {},
   "source": [
    "# Pierwszy checkpoint - walidacja Fake News Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c74589b-6c74-436a-903a-9633685abd60",
   "metadata": {},
   "source": [
    "### Wojtek Grabias, Wiktor Wierzchowski"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9a9064f-1175-4e2f-8c20-1dd9b199e02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('original_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca806809-3a71-43ea-aab5-f823e7a1b269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>Ground Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Ann Coulter Make Believes She Has ‘Gay Friend...</td>\n",
       "      <td>It s hard to believe, but Donald Trump does ha...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Rating: Moody‘s verbessert Ausblick für Russla...</td>\n",
       "      <td>bankensektor Der russische Staat werde die Ban...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>CAN WE ADD DIRTY MONEY ‘LAUNDERING’ To The Oba...</td>\n",
       "      <td>A member of the House Intelligence Committee i...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Republicans on Obamacare repeal: 'We're going ...</td>\n",
       "      <td>WASHINGTON (Reuters) - House of Representative...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Trump, on possible DACA deal, says border wall...</td>\n",
       "      <td>WASHINGTON (Reuters) - U.S. President Donald T...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69040</th>\n",
       "      <td>69040</td>\n",
       "      <td>Burundi opposition platform boycotts new round...</td>\n",
       "      <td>NAIROBI (Reuters) - Burundi s main opposition ...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69041</th>\n",
       "      <td>69041</td>\n",
       "      <td>Hillary’s Message To Former Miss Universe Cal...</td>\n",
       "      <td>Miss Universe 1996 Alicia Machado is now an Am...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69042</th>\n",
       "      <td>69042</td>\n",
       "      <td>Cop Crashes Car And Runs Away When More Cops A...</td>\n",
       "      <td>The Daily Sheeple – by Ryan Banister \\r\\nAn aw...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69043</th>\n",
       "      <td>69043</td>\n",
       "      <td>Trump Stole An Idea From North Korean Propaga...</td>\n",
       "      <td>Jesus f*cking Christ our President* is a moron...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69044</th>\n",
       "      <td>69044</td>\n",
       "      <td>BREAKING: HILLARY CLINTON’S STATE DEPARTMENT G...</td>\n",
       "      <td>IF SHE S NOT TOAST NOW THEN WE RE IN BIGGER TR...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69045 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                              title  \\\n",
       "0               0   Ann Coulter Make Believes She Has ‘Gay Friend...   \n",
       "1               1  Rating: Moody‘s verbessert Ausblick für Russla...   \n",
       "2               2  CAN WE ADD DIRTY MONEY ‘LAUNDERING’ To The Oba...   \n",
       "3               3  Republicans on Obamacare repeal: 'We're going ...   \n",
       "4               4  Trump, on possible DACA deal, says border wall...   \n",
       "...           ...                                                ...   \n",
       "69040       69040  Burundi opposition platform boycotts new round...   \n",
       "69041       69041   Hillary’s Message To Former Miss Universe Cal...   \n",
       "69042       69042  Cop Crashes Car And Runs Away When More Cops A...   \n",
       "69043       69043   Trump Stole An Idea From North Korean Propaga...   \n",
       "69044       69044  BREAKING: HILLARY CLINTON’S STATE DEPARTMENT G...   \n",
       "\n",
       "                                                    text Ground Label  \n",
       "0      It s hard to believe, but Donald Trump does ha...         fake  \n",
       "1      bankensektor Der russische Staat werde die Ban...         fake  \n",
       "2      A member of the House Intelligence Committee i...         fake  \n",
       "3      WASHINGTON (Reuters) - House of Representative...         true  \n",
       "4      WASHINGTON (Reuters) - U.S. President Donald T...         true  \n",
       "...                                                  ...          ...  \n",
       "69040  NAIROBI (Reuters) - Burundi s main opposition ...         true  \n",
       "69041  Miss Universe 1996 Alicia Machado is now an Am...         fake  \n",
       "69042  The Daily Sheeple – by Ryan Banister \\r\\nAn aw...         fake  \n",
       "69043  Jesus f*cking Christ our President* is a moron...         fake  \n",
       "69044  IF SHE S NOT TOAST NOW THEN WE RE IN BIGGER TR...         fake  \n",
       "\n",
       "[69045 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718ac205",
   "metadata": {},
   "source": [
    "---\n",
    "## Trzeba też zrobić jakieś podsumowanie tego budowania w kontekście technicznym\n",
    "##### To co zrobili jest zrealizowane poprawnie i robi to, co założyli, że robi\n",
    "##### Podzielili poprawnie dane\n",
    "##### Dobre tam zapoznanie z ramką (chociażby te języki)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e2958d-0556-4c2a-b42a-56530c04ef9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUTAJ BĘDZIE KOD KTÓRY ROBI TO CO PISZE NIŻEJ ALE GO NIE DALI WIEC BEDE SZUKAŁ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236435f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Kod jest tutaj, wystarczy wywołać Main'a\n",
    "# Main jest podzielony na etapy więc można go sobie tam zmieniać fajnie\n",
    "# Ale tam dużo działania na jakichś csv to chyba średnio to w notebooku robić\n",
    "# Po prostu możemy powiedzieć no ta, działa bez kitu\n",
    "# https://github.com/kitqu/ML-projekt1/blob/main/cleaning_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6932f3b9-a6fd-4377-bf22-b41c303ed0ea",
   "metadata": {},
   "source": [
    "Po wstępnej analizie danych usunięcie języków innych niż angielski wydaje się w pełni uzasadnione.  \n",
    "Nie jesteśmy przekonani natomiast czy łączenie kolumn 'text' i 'title', mimo że ułatwi modelowi analize, nie pozbawia nas pewnych informacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a4e6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Też jestem tego zdania"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd56f05",
   "metadata": {},
   "source": [
    "Wydaje się, że wszystkie najważniejsze elementy przetworzenia tekstu na potrzeby klasyfikacji zostały uwzględnione. \n",
    "Jedyne co sugerowalibyśmy dodać to rozwinięcie potencjalnych skrótów obecnych w języku angielskim. np 'm, 're, 't, ain't itd. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babc5d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Nawet bym je policzył w kolumnie \"text\" (konkretniej - ilość \"skrótów\" w stosunku\n",
    "# do długości tekstu), ale uwaga na \" 's \" - nie zamieniałbym (określa przynależność, w dużej ilości\n",
    "# przypadków źle zamieni)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8884b3b5-a910-4728-8ce6-c964f432d49f",
   "metadata": {},
   "source": [
    " \n",
    "Zastanawia nas jednak czy poprawnym w kontekście analizy fake newsów jest standaryzacja wielkości liter. Jesteśmy skłonni uznać, że nadmierne stosowanie CapsLock'a, szczególnie w tytułach artykułów, może być uznane za potencjalny predyktor nierzetelności (stąd sugestia pozostawienia title i text oddzielnie)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651eb58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Zgadzam się\n",
    "# Może też dodać stosunek słów napisanych CapsLockiem do długości tytułu\n",
    "# Albo ogólnie wielkich liter do długości tytułu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb4667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Czyli ogólnie pomysł jest\n",
    "# Zostawienie podziału\n",
    "# Skróty dla kolumny text\n",
    "# CapsLock dla kolumny title\n",
    "\n",
    "# Mój pomysł: Wstawić dwie kolumny:\n",
    "# Uppercase_ratio: stosunek ilości wielkich liter do długości tytułu\n",
    "# Abbrev_ratio: stosunek ilości skrótów do długości tekstu\n",
    "\n",
    "# A następnie je wszystkie sprowadzić do parteru tak jak oni robili - w  ten sposób dostaniemy info\n",
    "# na temat samych słów, a nie to, jak zostały napisane (przestaniemy już odróżniać GÓWNA od gowno)\n",
    "# ale o to nam w sumie chodzi, po co dalej je traktować jako osobne byty skoro już mamy zapisaną informacje\n",
    "# o tym, że GÓWNA jest capslockiem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c0427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Liczy uppercase\n",
    "def n_upper_chars(string):\n",
    "    return sum(map(str.isupper, string))\n",
    "# Liczy stosunek\n",
    "def upper_ratio(string):\n",
    "    return n_upper_chars(string)/len(string.replace(\" \",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7430d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Liczenie skrótów zajebałem nie bo po co koło na nowo odkrywać"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fde8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "contractions = {\"ain't\": [\"am not\", \"are not\", \"is not\", \"has not\", \"have not\"], \"aren't\": [\"are not\", \"am not\"], \"i've\": [\"i have\", ]} # ...\n",
    "\n",
    "# with re you can define advanced regexes, but maybe\n",
    "# from string import punctuation (suggestion from Martijn Pieters answer\n",
    "# is still enough for you)\n",
    "import re\n",
    "\n",
    "def abbreviation_counter(input_text, abbreviation_dict):   \n",
    "    count = 0\n",
    "    # what you want is a list of words. str.split() does this job for you.\n",
    "    # \" \" is default and you can also omit this. But if you really need better\n",
    "    # methods (see answer text abover), you have to take a word tokenizer tool\n",
    "    # or have to write your own.\n",
    "    for word in input_text.split(\" \"):\n",
    "        # and also clean word (remove ',', ';', ...) afterwards. The advantage of \n",
    "        # using re over `from string import punctuation` is that you have more\n",
    "        # control in what you want to remove. That means that you can add or\n",
    "        # remove easily any punctuation mark. It could be very handy. It could be\n",
    "        # also overpowered. If the latter is the case, just stick to Martijn Pieters\n",
    "        # solution.\n",
    "        if re.sub(',|;', '', word).lower() in abbreviation_dict:\n",
    "            count += 1\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe60d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# No i te powyższe funkcje apply damy na te kolumny tworząc nowe, powinno grać i buczeć"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05306094-97cc-443e-978d-5727dca004d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "# funkcja odpowiedzialna za czyszczenie pojedynczego wiersza\n",
    "\n",
    "def clean_text(text, punctuation_chars):\n",
    "    # usuwamy zamianę na małe litery\n",
    "    # text = text.lower()\n",
    "    # rozwinięcie skrótów\n",
    "    text = expand_contractions(text)\n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', ''.join(punctuation_chars)))\n",
    "    # remove digits\n",
    "    text = text.translate(str.maketrans('', '', string.digits))\n",
    "    # remove all single characters\n",
    "    pattern = r'(^| ).( |$)'\n",
    "    text = re.sub(pattern, ' ', text)\n",
    "    # remove multiple spaces\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    # remove stopwords\n",
    "    text = delete_stopwords(text)\n",
    "    # stemming\n",
    "    text = stemming(text)\n",
    "    return text\n",
    "\n",
    "# funkcja odpowiedzialna za usuwanie stopwordów\n",
    "\n",
    "def delete_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# funkcja odpowiedzialna za stemizację\n",
    "\n",
    "def stemming(text):\n",
    "    words = word_tokenize(text)\n",
    "    porter = PorterStemmer()\n",
    "    stem_words = [porter.stem(word) for word in words]\n",
    "    return ' '.join(stem_words)\n",
    "\n",
    "# funkcja odpowiedzialna za rozwijanie skrótów (zajebana z internetów, nie mam pojęcia o co chodzi z \"match\")\n",
    "contraction_dict = {\"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\"}\n",
    "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "\n",
    "def expand_contractions(text, contraction_dict = contraction_dict)\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return contractions_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0b5e39-a830-46de-8682-5fb457cbe626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUTAJ UŻYCIE TEGO ^^^ POKAZANIE: ŚMIGA BAJLANDO ESSUNIA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c024604-aee6-4482-ab52-88e7cd3b7de1",
   "metadata": {},
   "source": [
    "## brudno pis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea120899-c40e-4700-a70e-c03850a6b23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_dict = {\"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\"}\n",
    "\n",
    "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "\n",
    "def expand_contractions(text,contractions_dict=contractions_dict):\n",
    "    \n",
    "    def replace(match):\n",
    "        \n",
    "        return contractions_dict[match.group(0)]\n",
    "    \n",
    "    return contractions_re.sub(replace, text)\n",
    "\n",
    "data['reviews.text']=data['reviews.text'].apply(lambda x:expand_contractions(x))\n",
    "\n",
    "# expand contraction function uses a regular expression to map the contraction to the word. \n",
    "# It will match the word with keys and if it is present then replace that word with its value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc10b4ff",
   "metadata": {},
   "source": [
    "Notka do wojtusia: Nie wiem dokładnie co dalej. Jak masz już ogarnięte dane textowe to musisz zamienić je na liczby i do tego można użyć kilku metod. Oni użyli tej tf-idf... no i jakby git ale ciężko powiedzieć czy coś innego było by lepsze zanim nie sprawdzisz tego modelem. Znalazłem, że można jeszcze pierdolnąć bag-of-words(to jest łatwe chyba) albo word embeddings(tego nie patrzyłem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9113a0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Nie wiem na ile bag-of-words jest fajne (to trochę taki one-hot, wchuj kolumn będzie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bcb3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Tutaj implementacja word embeddings\n",
    "# Jeszcze tego nie testowałem ale działa na 99% bo to z dokumentacji zajebałem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabdac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "'''\n",
    "import gensim\n",
    "\n",
    "sentences = [nltk.word_tokenize(text) for text in df['text']] \n",
    "model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "vector = []\n",
    "for text in df['text']:\n",
    "    words = nltk.word_tokenize(text)\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv.vocab]\n",
    "    if len(vectors) > 0:\n",
    "        vector.append(np.mean(vectors, axis=0))\n",
    "    else:\n",
    "        vector.append(np.zeros(model.vector_size))\n",
    "vector = np.array(vector)\n",
    "# Oni też zostawiali jako wektor więc wyjebane niech się męczą\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c22d501-2bcb-44bd-b5c6-09dfbea64f8f",
   "metadata": {},
   "source": [
    "Także zostawiam to na dziś, popatrz, pomyśl, na moje to jeśli chodzi o to co na górze, to to co napisałem to jedyne do czego można się przyjebać. Też daj feedback czy nie jest to głupie z tymi kolumnami i dużymi literkami. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c478d859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# <3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
